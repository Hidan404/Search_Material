{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: python\n",
      "N√∫mero de PDFs: 5\n",
      "Qualidade: alta (Tipo: <class 'str'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ronal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ronal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "usage: ipykernel_launcher.py [-h] [-n NUM_PDFS] [-q QUALITY] subject\n",
      "ipykernel_launcher.py: error: argument -q/--quality: invalid float value: 'alta'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import requests\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from googlesearch import search\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, PageBreak, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Se necess√°rio, redefina sys.argv para evitar conflitos com argumentos do kernel:\n",
    "sys.argv = ['ipykernel_launcher.py', 'python', '-n', '5', '-q', 'alta']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('subject', nargs='?', default='assunto_padrao', help='Assunto para pesquisa')\n",
    "parser.add_argument('-n', '--num-pdfs', type=int, default=10, help='N√∫mero de PDFs')\n",
    "parser.add_argument('-q', '--quality', type=str, default='alta', help='Qualidade da pesquisa')  # Agora sempre string!\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"Subject: {args.subject}\")\n",
    "print(f\"N√∫mero de PDFs: {args.num_pdfs}\")\n",
    "print(f\"Qualidade: {args.quality} (Tipo: {type(args.quality)})\") \n",
    "\n",
    "class AdvancedKnowledgeAggregator:\n",
    "    def __init__(self, subject):\n",
    "        self.subject = subject\n",
    "        self.base_dir = Path(\"knowledge_books\") / subject.replace(\" \", \"_\")\n",
    "        self.raw_pdfs = self.base_dir / \"raw_pdfs\"\n",
    "        self.processed_texts = []\n",
    "        self.metadata = []\n",
    "        self.structure = {}\n",
    "        self.quality_scores = {}\n",
    "        \n",
    "        self._setup_directories()\n",
    "        self._load_config()\n",
    "\n",
    "    def _setup_directories(self):\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.raw_pdfs.mkdir(exist_ok=True)\n",
    "\n",
    "    def _load_config(self):\n",
    "        config_path = Path(\"config.yaml\")\n",
    "        if config_path.exists():\n",
    "            with open(config_path) as f:\n",
    "                self.config = yaml.safe_load(f)\n",
    "        else:\n",
    "            self.config = {\n",
    "                'structure': {\n",
    "                    'Introdu√ß√£o': ['Hist√≥rico', 'Conceitos B√°sicos'],\n",
    "                    'Desenvolvimento': ['T√©cnicas', 'Aplica√ß√µes'],\n",
    "                    'Conclus√£o': ['Resumo', 'Pr√≥ximos Passos']\n",
    "                },\n",
    "                'apis': {\n",
    "                    'crossref': {'enable': True, 'max_results': 5},\n",
    "                    'arxiv': {'enable': False}\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def search_content(self, num_files=15):\n",
    "        \"\"\"Busca conte√∫do em m√∫ltiplas fontes\"\"\"\n",
    "        self._search_web_pdfs(num_files)\n",
    "        if self.config['apis']['crossref']['enable']:\n",
    "            self._search_academic_papers()\n",
    "\n",
    "    def _search_web_pdfs(self, num_files):\n",
    "        query = f\"{self.subject} filetype:pdf\"\n",
    "        try:\n",
    "            results = search(\n",
    "                query,\n",
    "                num_results=num_files,\n",
    "                lang='pt' if 'brasil' in self.subject.lower() else 'en',\n",
    "                user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "            )\n",
    "            \n",
    "            for url in results:\n",
    "                if url.endswith('.pdf'):\n",
    "                    self._download_pdf(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na pesquisa web: {e}\")\n",
    "\n",
    "    def _search_academic_papers(self):\n",
    "        \"\"\"Integra√ß√£o com API CrossRef para artigos acad√™micos\"\"\"\n",
    "        url = \"https://api.crossref.org/works\"\n",
    "        params = {\n",
    "            'query': self.subject,\n",
    "            'filter': 'type:journal-article',\n",
    "            'rows': self.config['apis']['crossref']['max_results']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                for item in response.json()['message']['items']:\n",
    "                    if 'link' in item and item['link'][0]['content-type'] == 'application/pdf':\n",
    "                        self._download_pdf(item['link'][0]['URL'])\n",
    "                        self.metadata.append({\n",
    "                            'title': item.get('title', [''])[0],\n",
    "                            'authors': [author['given'] + ' ' + author['family'] for author in item.get('author', [])],\n",
    "                            'doi': item.get('DOI', ''),\n",
    "                            'year': item.get('created', {}).get('date-parts', [[2000]])[0][0]\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na API CrossRef: {e}\")\n",
    "\n",
    "    def _download_pdf(self, url):\n",
    "        \"\"\"Sistema de download com verifica√ß√£o de qualidade\"\"\"\n",
    "        try:\n",
    "            filename = url.split('/')[-1][:100] + \".pdf\"\n",
    "            filepath = self.raw_pdfs / filename\n",
    "            \n",
    "            if filepath.exists():\n",
    "                return\n",
    "\n",
    "            response = requests.get(url, stream=True, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                self._assess_quality(filepath)\n",
    "                print(f\"‚úÖ Baixado: {filename} [Score: {self.quality_scores.get(filename, 0):.1f}]\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar {url}: {e}\")\n",
    "\n",
    "    def _assess_quality(self, filepath):\n",
    "        \"\"\"Avalia√ß√£o de qualidade do PDF\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages[:5])\n",
    "                \n",
    "                # Crit√©rios de qualidade\n",
    "                length_score = min(len(text)/1000, 5)\n",
    "                keyword_score = sum(1 for w in ['introduction', 'method', 'conclusion'] if w in text.lower())\n",
    "                self.quality_scores[filepath.name] = length_score + keyword_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na avalia√ß√£o de qualidade: {e}\")\n",
    "            self.quality_scores[filepath.name] = 0\n",
    "\n",
    "    def process_content(self):\n",
    "        \"\"\"Processamento avan√ßado com limpeza e an√°lise\"\"\"\n",
    "        for pdf_file in sorted(self.raw_pdfs.glob(\"*.pdf\"), \n",
    "                             key=lambda x: self.quality_scores.get(x.name, 0), \n",
    "                             reverse=True):\n",
    "            try:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages)\n",
    "                clean_text = self._clean_text(text)\n",
    "                \n",
    "                if len(clean_text) > 500:  # Ignorar PDFs sem texto √∫til\n",
    "                    self.processed_texts.append(clean_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {pdf_file.name}: {e}\")\n",
    "\n",
    "        if self.processed_texts:\n",
    "            self._analyze_topics()\n",
    "            self._generate_index()\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Limpeza avan√ßada do texto\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "        text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)  # Remover repeti√ß√µes\n",
    "        return text.lower()\n",
    "\n",
    "    def _analyze_topics(self):\n",
    "        \"\"\"An√°lise sem√¢ntica com LDA\"\"\"\n",
    "        texts = [text.split() for text in self.processed_texts]\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        \n",
    "        lda_model = models.LdaModel(\n",
    "            corpus,\n",
    "            num_topics=5,\n",
    "            id2word=dictionary,\n",
    "            passes=15,\n",
    "            alpha='auto'\n",
    "        )\n",
    "        \n",
    "        self.structure = {}\n",
    "        for idx, topic in lda_model.print_topics(-1):\n",
    "            top_terms = re.findall(r'\"(\\w+)\"', topic)[:3]\n",
    "            self.structure[f\"Cap√≠tulo {idx+1}: {' '.join(top_terms)}\"] = []\n",
    "\n",
    "    def _generate_index(self):\n",
    "        \"\"\"Gera√ß√£o de √≠ndice remissivo\"\"\"\n",
    "        all_text = ' '.join(self.processed_texts)\n",
    "        words = nltk.word_tokenize(all_text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [w for w in words if w not in stop_words and len(w) > 4 and w.isalpha()]\n",
    "        \n",
    "        self.index = FreqDist(filtered_words).most_common(50)\n",
    "\n",
    "    def generate_book(self):\n",
    "        \"\"\"Gera√ß√£o do PDF com estrutura profissional\"\"\"\n",
    "        output_file = self.base_dir / f\"{self.subject}_{datetime.now().strftime('%Y%m%d')}.pdf\"\n",
    "        \n",
    "        doc = SimpleDocTemplate(str(output_file), pagesize=letter)\n",
    "        styles = self._create_styles()\n",
    "        story = []\n",
    "        \n",
    "        # Capa\n",
    "        story += self._create_cover(styles)\n",
    "        \n",
    "        # Metadados\n",
    "        story += self._create_metadata_table(styles)\n",
    "        \n",
    "        # Sum√°rio\n",
    "        story += self._create_toc(styles)\n",
    "        \n",
    "        # Conte√∫do principal\n",
    "        story += self._create_main_content(styles)\n",
    "        \n",
    "        # √çndice Remissivo\n",
    "        story += self._create_index(styles)\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"\\nüìï Livro gerado com sucesso em: {output_file}\")\n",
    "\n",
    "    def _create_styles(self):\n",
    "        \"\"\"Configura√ß√£o de estilos avan√ßados\"\"\"\n",
    "        styles = getSampleStyleSheet()\n",
    "        styles.add(ParagraphStyle(\n",
    "            name='ChapterTitle',\n",
    "            fontSize=16,\n",
    "            leading=18,\n",
    "            spaceAfter=20,\n",
    "            textColor=colors.darkblue\n",
    "        ))\n",
    "        return styles\n",
    "\n",
    "    def _create_cover(self, styles):\n",
    "        \"\"\"Cria√ß√£o da capa profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(f\"<b>{self.subject.upper()}</b>\", styles['Title']))\n",
    "        elements.append(Paragraph(\"<br/><br/>Relat√≥rio Gerado Automaticamente<br/>por Knowledge Aggregator 2.0\", styles['Italic']))\n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_metadata_table(self, styles):\n",
    "        \"\"\"Tabela de metadados dos artigos\"\"\"\n",
    "        data = [['T√≠tulo', 'Autores', 'Ano', 'DOI']]\n",
    "        for meta in self.metadata:\n",
    "            data.append([\n",
    "                meta['title'][:50],\n",
    "                ', '.join(meta['authors'][:3]),\n",
    "                str(meta['year']),\n",
    "                meta['doi'][:20]\n",
    "            ])\n",
    "        \n",
    "        table = Table(data, colWidths=[120, 120, 50, 100])\n",
    "        table.setStyle(TableStyle([\n",
    "            ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),\n",
    "            ('GRID', (0,0), (-1,-1), 1, colors.black),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 8)\n",
    "        ]))\n",
    "        \n",
    "        return [Paragraph(\"<b>Fontes Acad√™micas Utilizadas</b>\", styles['Heading2']), table, PageBreak()]\n",
    "\n",
    "    def _create_toc(self, styles):\n",
    "        \"\"\"Sum√°rio autom√°tico com numera√ß√£o\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"Sum√°rio\", styles['Heading1']))\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(f\"‚Ä¢ {chapter}\", styles['Normal']))\n",
    "        \n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_main_content(self, styles):\n",
    "        \"\"\"Conte√∫do estruturado com an√°lise sem√¢ntica\"\"\"\n",
    "        elements = []\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(self.processed_texts)\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(chapter, styles['ChapterTitle']))\n",
    "            \n",
    "            # Selecionar conte√∫do mais relevante\n",
    "            chapter_keywords = chapter.lower().split()[1:]\n",
    "            relevant_content = max(\n",
    "                self.processed_texts,\n",
    "                key=lambda x: sum(1 for kw in chapter_keywords if kw in x)\n",
    "            )\n",
    "            \n",
    "            elements.append(Paragraph(relevant_content[:1500] + \"...\", styles['Normal']))\n",
    "            elements.append(PageBreak())\n",
    "        \n",
    "        return elements\n",
    "\n",
    "    def _create_index(self, styles):\n",
    "        \"\"\"√çndice remissivo profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"√çndice Remissivo\", styles['Heading1']))\n",
    "        \n",
    "        index_items = []\n",
    "        for term, freq in self.index:\n",
    "            index_items.append(f\"{term} ({freq})\")\n",
    "        \n",
    "        columns = 3\n",
    "        table_data = []\n",
    "        for i in range(0, len(index_items), columns):\n",
    "            table_data.append(index_items[i:i+columns])\n",
    "        \n",
    "        table = Table(table_data)\n",
    "        table.setStyle(TableStyle([\n",
    "            ('ALIGN', (0,0), (-1,-1), 'LEFT'),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 9)\n",
    "        ]))\n",
    "        \n",
    "        return [table]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Advanced Knowledge Aggregator\")\n",
    "    parser.add_argument(\"subject\", type=str, help=\"Assunto principal do livro\")\n",
    "    parser.add_argument(\"-n\", \"--num-pdfs\", type=int, default=10, help=\"N√∫mero de PDFs para baixar\")\n",
    "    parser.add_argument(\"-q\", \"--quality\", type=float, default=3.0, help=\"Limite m√≠nimo de qualidade (0-5)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(f\"üöÄ Iniciando agrega√ß√£o de conhecimento sobre: {args.subject}\")\n",
    "    aggregator = AdvancedKnowledgeAggregator(args.subject)\n",
    "    \n",
    "    print(\"\\nüîç Buscando conte√∫do relevante...\")\n",
    "    aggregator.search_content(args.num_pdfs)\n",
    "    \n",
    "    print(\"\\nüß† Processando e analisando conte√∫do...\")\n",
    "    aggregator.process_content()\n",
    "    \n",
    "    if aggregator.processed_texts:\n",
    "        print(\"\\nüìö Gerando livro estruturado...\")\n",
    "        aggregator.generate_book()\n",
    "    else:\n",
    "        print(\"‚ùå Nenhum conte√∫do v√°lido encontrado para gera√ß√£o do livro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests PyPDF2 googlesearch-python reportlab nltk gensim scikit-learn pyyaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
