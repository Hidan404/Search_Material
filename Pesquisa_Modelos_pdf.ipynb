{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: python\n",
      "Número de PDFs: 5\n",
      "Qualidade: alta (Tipo: <class 'str'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ronal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ronal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "usage: ipykernel_launcher.py [-h] [-n NUM_PDFS] [-q QUALITY] subject\n",
      "ipykernel_launcher.py: error: argument -q/--quality: invalid float value: 'alta'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import requests\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from googlesearch import search\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, PageBreak, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Se necessário, redefina sys.argv para evitar conflitos com argumentos do kernel:\n",
    "sys.argv = ['ipykernel_launcher.py', 'python', '-n', '5', '-q', 'alta']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('subject', nargs='?', default='assunto_padrao', help='Assunto para pesquisa')\n",
    "parser.add_argument('-n', '--num-pdfs', type=int, default=10, help='Número de PDFs')\n",
    "parser.add_argument('-q', '--quality', type=str, default='alta', help='Qualidade da pesquisa')  # Agora sempre string!\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"Subject: {args.subject}\")\n",
    "print(f\"Número de PDFs: {args.num_pdfs}\")\n",
    "print(f\"Qualidade: {args.quality} (Tipo: {type(args.quality)})\") \n",
    "\n",
    "class AdvancedKnowledgeAggregator:\n",
    "    def __init__(self, subject):\n",
    "        self.subject = subject\n",
    "        self.base_dir = Path(\"knowledge_books\") / subject.replace(\" \", \"_\")\n",
    "        self.raw_pdfs = self.base_dir / \"raw_pdfs\"\n",
    "        self.processed_texts = []\n",
    "        self.metadata = []\n",
    "        self.structure = {}\n",
    "        self.quality_scores = {}\n",
    "        \n",
    "        self._setup_directories()\n",
    "        self._load_config()\n",
    "\n",
    "    def _setup_directories(self):\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.raw_pdfs.mkdir(exist_ok=True)\n",
    "\n",
    "    def _load_config(self):\n",
    "        config_path = Path(\"config.yaml\")\n",
    "        if config_path.exists():\n",
    "            with open(config_path) as f:\n",
    "                self.config = yaml.safe_load(f)\n",
    "        else:\n",
    "            self.config = {\n",
    "                'structure': {\n",
    "                    'Introdução': ['Histórico', 'Conceitos Básicos'],\n",
    "                    'Desenvolvimento': ['Técnicas', 'Aplicações'],\n",
    "                    'Conclusão': ['Resumo', 'Próximos Passos']\n",
    "                },\n",
    "                'apis': {\n",
    "                    'crossref': {'enable': True, 'max_results': 5},\n",
    "                    'arxiv': {'enable': False}\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def search_content(self, num_files=15):\n",
    "        \"\"\"Busca conteúdo em múltiplas fontes\"\"\"\n",
    "        self._search_web_pdfs(num_files)\n",
    "        if self.config['apis']['crossref']['enable']:\n",
    "            self._search_academic_papers()\n",
    "\n",
    "    def _search_web_pdfs(self, num_files):\n",
    "        query = f\"{self.subject} filetype:pdf\"\n",
    "        try:\n",
    "            results = search(\n",
    "                query,\n",
    "                num_results=num_files,\n",
    "                lang='pt' if 'brasil' in self.subject.lower() else 'en',\n",
    "                user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "            )\n",
    "            \n",
    "            for url in results:\n",
    "                if url.endswith('.pdf'):\n",
    "                    self._download_pdf(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na pesquisa web: {e}\")\n",
    "\n",
    "    def _search_academic_papers(self):\n",
    "        \"\"\"Integração com API CrossRef para artigos acadêmicos\"\"\"\n",
    "        url = \"https://api.crossref.org/works\"\n",
    "        params = {\n",
    "            'query': self.subject,\n",
    "            'filter': 'type:journal-article',\n",
    "            'rows': self.config['apis']['crossref']['max_results']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                for item in response.json()['message']['items']:\n",
    "                    if 'link' in item and item['link'][0]['content-type'] == 'application/pdf':\n",
    "                        self._download_pdf(item['link'][0]['URL'])\n",
    "                        self.metadata.append({\n",
    "                            'title': item.get('title', [''])[0],\n",
    "                            'authors': [author['given'] + ' ' + author['family'] for author in item.get('author', [])],\n",
    "                            'doi': item.get('DOI', ''),\n",
    "                            'year': item.get('created', {}).get('date-parts', [[2000]])[0][0]\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na API CrossRef: {e}\")\n",
    "\n",
    "    def _download_pdf(self, url):\n",
    "        \"\"\"Sistema de download com verificação de qualidade\"\"\"\n",
    "        try:\n",
    "            filename = url.split('/')[-1][:100] + \".pdf\"\n",
    "            filepath = self.raw_pdfs / filename\n",
    "            \n",
    "            if filepath.exists():\n",
    "                return\n",
    "\n",
    "            response = requests.get(url, stream=True, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                self._assess_quality(filepath)\n",
    "                print(f\"✅ Baixado: {filename} [Score: {self.quality_scores.get(filename, 0):.1f}]\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar {url}: {e}\")\n",
    "\n",
    "    def _assess_quality(self, filepath):\n",
    "        \"\"\"Avaliação de qualidade do PDF\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages[:5])\n",
    "                \n",
    "                # Critérios de qualidade\n",
    "                length_score = min(len(text)/1000, 5)\n",
    "                keyword_score = sum(1 for w in ['introduction', 'method', 'conclusion'] if w in text.lower())\n",
    "                self.quality_scores[filepath.name] = length_score + keyword_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na avaliação de qualidade: {e}\")\n",
    "            self.quality_scores[filepath.name] = 0\n",
    "\n",
    "    def process_content(self):\n",
    "        \"\"\"Processamento avançado com limpeza e análise\"\"\"\n",
    "        for pdf_file in sorted(self.raw_pdfs.glob(\"*.pdf\"), \n",
    "                             key=lambda x: self.quality_scores.get(x.name, 0), \n",
    "                             reverse=True):\n",
    "            try:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages)\n",
    "                clean_text = self._clean_text(text)\n",
    "                \n",
    "                if len(clean_text) > 500:  # Ignorar PDFs sem texto útil\n",
    "                    self.processed_texts.append(clean_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {pdf_file.name}: {e}\")\n",
    "\n",
    "        if self.processed_texts:\n",
    "            self._analyze_topics()\n",
    "            self._generate_index()\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Limpeza avançada do texto\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "        text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)  # Remover repetições\n",
    "        return text.lower()\n",
    "\n",
    "    def _analyze_topics(self):\n",
    "        \"\"\"Análise semântica com LDA\"\"\"\n",
    "        texts = [text.split() for text in self.processed_texts]\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        \n",
    "        lda_model = models.LdaModel(\n",
    "            corpus,\n",
    "            num_topics=5,\n",
    "            id2word=dictionary,\n",
    "            passes=15,\n",
    "            alpha='auto'\n",
    "        )\n",
    "        \n",
    "        self.structure = {}\n",
    "        for idx, topic in lda_model.print_topics(-1):\n",
    "            top_terms = re.findall(r'\"(\\w+)\"', topic)[:3]\n",
    "            self.structure[f\"Capítulo {idx+1}: {' '.join(top_terms)}\"] = []\n",
    "\n",
    "    def _generate_index(self):\n",
    "        \"\"\"Geração de índice remissivo\"\"\"\n",
    "        all_text = ' '.join(self.processed_texts)\n",
    "        words = nltk.word_tokenize(all_text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [w for w in words if w not in stop_words and len(w) > 4 and w.isalpha()]\n",
    "        \n",
    "        self.index = FreqDist(filtered_words).most_common(50)\n",
    "\n",
    "    def generate_book(self):\n",
    "        \"\"\"Geração do PDF com estrutura profissional\"\"\"\n",
    "        output_file = self.base_dir / f\"{self.subject}_{datetime.now().strftime('%Y%m%d')}.pdf\"\n",
    "        \n",
    "        doc = SimpleDocTemplate(str(output_file), pagesize=letter)\n",
    "        styles = self._create_styles()\n",
    "        story = []\n",
    "        \n",
    "        # Capa\n",
    "        story += self._create_cover(styles)\n",
    "        \n",
    "        # Metadados\n",
    "        story += self._create_metadata_table(styles)\n",
    "        \n",
    "        # Sumário\n",
    "        story += self._create_toc(styles)\n",
    "        \n",
    "        # Conteúdo principal\n",
    "        story += self._create_main_content(styles)\n",
    "        \n",
    "        # Índice Remissivo\n",
    "        story += self._create_index(styles)\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"\\n📕 Livro gerado com sucesso em: {output_file}\")\n",
    "\n",
    "    def _create_styles(self):\n",
    "        \"\"\"Configuração de estilos avançados\"\"\"\n",
    "        styles = getSampleStyleSheet()\n",
    "        styles.add(ParagraphStyle(\n",
    "            name='ChapterTitle',\n",
    "            fontSize=16,\n",
    "            leading=18,\n",
    "            spaceAfter=20,\n",
    "            textColor=colors.darkblue\n",
    "        ))\n",
    "        return styles\n",
    "\n",
    "    def _create_cover(self, styles):\n",
    "        \"\"\"Criação da capa profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(f\"<b>{self.subject.upper()}</b>\", styles['Title']))\n",
    "        elements.append(Paragraph(\"<br/><br/>Relatório Gerado Automaticamente<br/>por Knowledge Aggregator 2.0\", styles['Italic']))\n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_metadata_table(self, styles):\n",
    "        \"\"\"Tabela de metadados dos artigos\"\"\"\n",
    "        data = [['Título', 'Autores', 'Ano', 'DOI']]\n",
    "        for meta in self.metadata:\n",
    "            data.append([\n",
    "                meta['title'][:50],\n",
    "                ', '.join(meta['authors'][:3]),\n",
    "                str(meta['year']),\n",
    "                meta['doi'][:20]\n",
    "            ])\n",
    "        \n",
    "        table = Table(data, colWidths=[120, 120, 50, 100])\n",
    "        table.setStyle(TableStyle([\n",
    "            ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),\n",
    "            ('GRID', (0,0), (-1,-1), 1, colors.black),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 8)\n",
    "        ]))\n",
    "        \n",
    "        return [Paragraph(\"<b>Fontes Acadêmicas Utilizadas</b>\", styles['Heading2']), table, PageBreak()]\n",
    "\n",
    "    def _create_toc(self, styles):\n",
    "        \"\"\"Sumário automático com numeração\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"Sumário\", styles['Heading1']))\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(f\"• {chapter}\", styles['Normal']))\n",
    "        \n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_main_content(self, styles):\n",
    "        \"\"\"Conteúdo estruturado com análise semântica\"\"\"\n",
    "        elements = []\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(self.processed_texts)\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(chapter, styles['ChapterTitle']))\n",
    "            \n",
    "            # Selecionar conteúdo mais relevante\n",
    "            chapter_keywords = chapter.lower().split()[1:]\n",
    "            relevant_content = max(\n",
    "                self.processed_texts,\n",
    "                key=lambda x: sum(1 for kw in chapter_keywords if kw in x)\n",
    "            )\n",
    "            \n",
    "            elements.append(Paragraph(relevant_content[:1500] + \"...\", styles['Normal']))\n",
    "            elements.append(PageBreak())\n",
    "        \n",
    "        return elements\n",
    "\n",
    "    def _create_index(self, styles):\n",
    "        \"\"\"Índice remissivo profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"Índice Remissivo\", styles['Heading1']))\n",
    "        \n",
    "        index_items = []\n",
    "        for term, freq in self.index:\n",
    "            index_items.append(f\"{term} ({freq})\")\n",
    "        \n",
    "        columns = 3\n",
    "        table_data = []\n",
    "        for i in range(0, len(index_items), columns):\n",
    "            table_data.append(index_items[i:i+columns])\n",
    "        \n",
    "        table = Table(table_data)\n",
    "        table.setStyle(TableStyle([\n",
    "            ('ALIGN', (0,0), (-1,-1), 'LEFT'),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 9)\n",
    "        ]))\n",
    "        \n",
    "        return [table]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Advanced Knowledge Aggregator\")\n",
    "    parser.add_argument(\"subject\", type=str, help=\"Assunto principal do livro\")\n",
    "    parser.add_argument(\"-n\", \"--num-pdfs\", type=int, default=10, help=\"Número de PDFs para baixar\")\n",
    "    parser.add_argument(\"-q\", \"--quality\", type=float, default=3.0, help=\"Limite mínimo de qualidade (0-5)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(f\"🚀 Iniciando agregação de conhecimento sobre: {args.subject}\")\n",
    "    aggregator = AdvancedKnowledgeAggregator(args.subject)\n",
    "    \n",
    "    print(\"\\n🔍 Buscando conteúdo relevante...\")\n",
    "    aggregator.search_content(args.num_pdfs)\n",
    "    \n",
    "    print(\"\\n🧠 Processando e analisando conteúdo...\")\n",
    "    aggregator.process_content()\n",
    "    \n",
    "    if aggregator.processed_texts:\n",
    "        print(\"\\n📚 Gerando livro estruturado...\")\n",
    "        aggregator.generate_book()\n",
    "    else:\n",
    "        print(\"❌ Nenhum conteúdo válido encontrado para geração do livro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests PyPDF2 googlesearch-python reportlab nltk gensim scikit-learn pyyaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
