{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import requests\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from googlesearch import search\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, PageBreak, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Se necessário, redefina sys.argv para evitar conflitos com argumentos do kernel:\n",
    "sys.argv = ['ipykernel_launcher.py', 'python', '-n', '5', '-q', 'alta']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('subject', nargs='?', default='assunto_padrao', help='Assunto para pesquisa')\n",
    "parser.add_argument('-n', '--num-pdfs', type=int, default=10, help='Número de PDFs')\n",
    "parser.add_argument('-q', '--quality', type=str, default='alta', help='Qualidade da pesquisa')  # Agora sempre string!\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"Subject: {args.subject}\")\n",
    "print(f\"Número de PDFs: {args.num_pdfs}\")\n",
    "print(f\"Qualidade: {args.quality} (Tipo: {type(args.quality)})\") \n",
    "\n",
    "class AdvancedKnowledgeAggregator:\n",
    "    def __init__(self, subject):\n",
    "        self.subject = subject\n",
    "        self.base_dir = Path(\"knowledge_books\") / subject.replace(\" \", \"_\")\n",
    "        self.raw_pdfs = self.base_dir / \"raw_pdfs\"\n",
    "        self.processed_texts = []\n",
    "        self.metadata = []\n",
    "        self.structure = {}\n",
    "        self.quality_scores = {}\n",
    "        \n",
    "        self._setup_directories()\n",
    "        self._load_config()\n",
    "\n",
    "    def _setup_directories(self):\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.raw_pdfs.mkdir(exist_ok=True)\n",
    "\n",
    "    def _load_config(self):\n",
    "        config_path = Path(\"config.yaml\")\n",
    "        if config_path.exists():\n",
    "            with open(config_path) as f:\n",
    "                self.config = yaml.safe_load(f)\n",
    "        else:\n",
    "            self.config = {\n",
    "                'structure': {\n",
    "                    'Introdução': ['Histórico', 'Conceitos Básicos'],\n",
    "                    'Desenvolvimento': ['Técnicas', 'Aplicações'],\n",
    "                    'Conclusão': ['Resumo', 'Próximos Passos']\n",
    "                },\n",
    "                'apis': {\n",
    "                    'crossref': {'enable': True, 'max_results': 5},\n",
    "                    'arxiv': {'enable': False}\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def search_content(self, num_files=15):\n",
    "        \"\"\"Busca conteúdo em múltiplas fontes\"\"\"\n",
    "        self._search_web_pdfs(num_files)\n",
    "        if self.config['apis']['crossref']['enable']:\n",
    "            self._search_academic_papers()\n",
    "\n",
    "    def _search_web_pdfs(self, num_files):\n",
    "        query = f\"{self.subject} filetype:pdf\"\n",
    "        try:\n",
    "            results = search(\n",
    "                query,\n",
    "                num_results=num_files,\n",
    "                lang='pt' if 'brasil' in self.subject.lower() else 'en',\n",
    "                user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "            )\n",
    "            \n",
    "            for url in results:\n",
    "                if url.endswith('.pdf'):\n",
    "                    self._download_pdf(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na pesquisa web: {e}\")\n",
    "\n",
    "    def _search_academic_papers(self):\n",
    "        \"\"\"Integração com API CrossRef para artigos acadêmicos\"\"\"\n",
    "        url = \"https://api.crossref.org/works\"\n",
    "        params = {\n",
    "            'query': self.subject,\n",
    "            'filter': 'type:journal-article',\n",
    "            'rows': self.config['apis']['crossref']['max_results']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                for item in response.json()['message']['items']:\n",
    "                    if 'link' in item and item['link'][0]['content-type'] == 'application/pdf':\n",
    "                        self._download_pdf(item['link'][0]['URL'])\n",
    "                        self.metadata.append({\n",
    "                            'title': item.get('title', [''])[0],\n",
    "                            'authors': [author['given'] + ' ' + author['family'] for author in item.get('author', [])],\n",
    "                            'doi': item.get('DOI', ''),\n",
    "                            'year': item.get('created', {}).get('date-parts', [[2000]])[0][0]\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na API CrossRef: {e}\")\n",
    "\n",
    "    def _download_pdf(self, url):\n",
    "        \"\"\"Sistema de download com verificação de qualidade\"\"\"\n",
    "        try:\n",
    "            filename = url.split('/')[-1][:100] + \".pdf\"\n",
    "            filepath = self.raw_pdfs / filename\n",
    "            \n",
    "            if filepath.exists():\n",
    "                return\n",
    "\n",
    "            response = requests.get(url, stream=True, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                self._assess_quality(filepath)\n",
    "                print(f\"✅ Baixado: {filename} [Score: {self.quality_scores.get(filename, 0):.1f}]\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar {url}: {e}\")\n",
    "\n",
    "    def _assess_quality(self, filepath):\n",
    "        \"\"\"Avaliação de qualidade do PDF\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages[:5])\n",
    "                \n",
    "                # Critérios de qualidade\n",
    "                length_score = min(len(text)/1000, 5)\n",
    "                keyword_score = sum(1 for w in ['introduction', 'method', 'conclusion'] if w in text.lower())\n",
    "                self.quality_scores[filepath.name] = length_score + keyword_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na avaliação de qualidade: {e}\")\n",
    "            self.quality_scores[filepath.name] = 0\n",
    "\n",
    "    def process_content(self):\n",
    "        \"\"\"Processamento avançado com limpeza e análise\"\"\"\n",
    "        for pdf_file in sorted(self.raw_pdfs.glob(\"*.pdf\"), \n",
    "                             key=lambda x: self.quality_scores.get(x.name, 0), \n",
    "                             reverse=True):\n",
    "            try:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages)\n",
    "                clean_text = self._clean_text(text)\n",
    "                \n",
    "                if len(clean_text) > 500:  # Ignorar PDFs sem texto útil\n",
    "                    self.processed_texts.append(clean_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {pdf_file.name}: {e}\")\n",
    "\n",
    "        if self.processed_texts:\n",
    "            self._analyze_topics()\n",
    "            self._generate_index()\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Limpeza avançada do texto\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "        text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)  # Remover repetições\n",
    "        return text.lower()\n",
    "\n",
    "   \n",
    "\n",
    "def _analyze_topics(self):\n",
    "    # Vetorização dos textos\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tf = vectorizer.fit_transform(self.processed_texts)\n",
    "    \n",
    "    # Aplicação do LDA\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=5,  # Número de tópicos\n",
    "        max_iter=10,\n",
    "        learning_method='online',\n",
    "        random_state=42\n",
    "    )\n",
    "    lda.fit(tf)\n",
    "    \n",
    "    # Extração dos tópicos\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    self.structure = {}\n",
    "    \n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        top_terms = [feature_names[i] for i in topic.argsort()[:-4:-1]]  # Top 3 termos\n",
    "        self.structure[f\"Capítulo {idx+1}: {' '.join(top_terms)}\"] = []\n",
    "\n",
    "    def _generate_index(self):\n",
    "        \"\"\"Geração de índice remissivo\"\"\"\n",
    "        all_text = ' '.join(self.processed_texts)\n",
    "        words = nltk.word_tokenize(all_text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [w for w in words if w not in stop_words and len(w) > 4 and w.isalpha()]\n",
    "        \n",
    "        self.index = FreqDist(filtered_words).most_common(50)\n",
    "\n",
    "    def generate_book(self):\n",
    "        \"\"\"Geração do PDF com estrutura profissional\"\"\"\n",
    "        output_file = self.base_dir / f\"{self.subject}_{datetime.now().strftime('%Y%m%d')}.pdf\"\n",
    "        \n",
    "        doc = SimpleDocTemplate(str(output_file), pagesize=letter)\n",
    "        styles = self._create_styles()\n",
    "        story = []\n",
    "        \n",
    "        # Capa\n",
    "        story += self._create_cover(styles)\n",
    "        \n",
    "        # Metadados\n",
    "        story += self._create_metadata_table(styles)\n",
    "        \n",
    "        # Sumário\n",
    "        story += self._create_toc(styles)\n",
    "        \n",
    "        # Conteúdo principal\n",
    "        story += self._create_main_content(styles)\n",
    "        \n",
    "        # Índice Remissivo\n",
    "        story += self._create_index(styles)\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"\\n📕 Livro gerado com sucesso em: {output_file}\")\n",
    "\n",
    "    def _create_styles(self):\n",
    "        \"\"\"Configuração de estilos avançados\"\"\"\n",
    "        styles = getSampleStyleSheet()\n",
    "        styles.add(ParagraphStyle(\n",
    "            name='ChapterTitle',\n",
    "            fontSize=16,\n",
    "            leading=18,\n",
    "            spaceAfter=20,\n",
    "            textColor=colors.darkblue\n",
    "        ))\n",
    "        return styles\n",
    "\n",
    "    def _create_cover(self, styles):\n",
    "        \"\"\"Criação da capa profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(f\"<b>{self.subject.upper()}</b>\", styles['Title']))\n",
    "        elements.append(Paragraph(\"<br/><br/>Relatório Gerado Automaticamente<br/>por Knowledge Aggregator 2.0\", styles['Italic']))\n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_metadata_table(self, styles):\n",
    "        \"\"\"Tabela de metadados dos artigos\"\"\"\n",
    "        data = [['Título', 'Autores', 'Ano', 'DOI']]\n",
    "        for meta in self.metadata:\n",
    "            data.append([\n",
    "                meta['title'][:50],\n",
    "                ', '.join(meta['authors'][:3]),\n",
    "                str(meta['year']),\n",
    "                meta['doi'][:20]\n",
    "            ])\n",
    "        \n",
    "        table = Table(data, colWidths=[120, 120, 50, 100])\n",
    "        table.setStyle(TableStyle([\n",
    "            ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),\n",
    "            ('GRID', (0,0), (-1,-1), 1, colors.black),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 8)\n",
    "        ]))\n",
    "        \n",
    "        return [Paragraph(\"<b>Fontes Acadêmicas Utilizadas</b>\", styles['Heading2']), table, PageBreak()]\n",
    "\n",
    "    def _create_toc(self, styles):\n",
    "        \"\"\"Sumário automático com numeração\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"Sumário\", styles['Heading1']))\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(f\"• {chapter}\", styles['Normal']))\n",
    "        \n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_main_content(self, styles):\n",
    "        \"\"\"Conteúdo estruturado com análise semântica\"\"\"\n",
    "        elements = []\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(self.processed_texts)\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(chapter, styles['ChapterTitle']))\n",
    "            \n",
    "            # Selecionar conteúdo mais relevante\n",
    "            chapter_keywords = chapter.lower().split()[1:]\n",
    "            relevant_content = max(\n",
    "                self.processed_texts,\n",
    "                key=lambda x: sum(1 for kw in chapter_keywords if kw in x)\n",
    "            )\n",
    "            \n",
    "            elements.append(Paragraph(relevant_content[:1500] + \"...\", styles['Normal']))\n",
    "            elements.append(PageBreak())\n",
    "        \n",
    "        return elements\n",
    "\n",
    "    def _create_index(self, styles):\n",
    "        \"\"\"Índice remissivo profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"Índice Remissivo\", styles['Heading1']))\n",
    "        \n",
    "        index_items = []\n",
    "        for term, freq in self.index:\n",
    "            index_items.append(f\"{term} ({freq})\")\n",
    "        \n",
    "        columns = 3\n",
    "        table_data = []\n",
    "        for i in range(0, len(index_items), columns):\n",
    "            table_data.append(index_items[i:i+columns])\n",
    "        \n",
    "        table = Table(table_data)\n",
    "        table.setStyle(TableStyle([\n",
    "            ('ALIGN', (0,0), (-1,-1), 'LEFT'),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 9)\n",
    "        ]))\n",
    "        \n",
    "        return [table]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Advanced Knowledge Aggregator\")\n",
    "    parser.add_argument(\"subject\", type=str, help=\"Assunto principal do livro\")\n",
    "    parser.add_argument(\"-n\", \"--num-pdfs\", type=int, default=10, help=\"Número de PDFs para baixar\")\n",
    "    parser.add_argument(\"-q\", \"--quality\", type=float, default=3.0, help=\"Limite mínimo de qualidade (0-5)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(f\"🚀 Iniciando agregação de conhecimento sobre: {args.subject}\")\n",
    "    aggregator = AdvancedKnowledgeAggregator(args.subject)\n",
    "    \n",
    "    print(\"\\n🔍 Buscando conteúdo relevante...\")\n",
    "    aggregator.search_content(args.num_pdfs)\n",
    "    \n",
    "    print(\"\\n🧠 Processando e analisando conteúdo...\")\n",
    "    aggregator.process_content()\n",
    "    \n",
    "    if aggregator.processed_texts:\n",
    "        print(\"\\n📚 Gerando livro estruturado...\")\n",
    "        aggregator.generate_book()\n",
    "    else:\n",
    "        print(\"❌ Nenhum conteúdo válido encontrado para geração do livro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/lib/python3.13/site-packages (2.32.3)\n",
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting googlesearch-python\n",
      "  Using cached googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting reportlab\n",
      "  Using cached reportlab-4.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/lib64/python3.13/site-packages (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/lib/python3.13/site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/lib64/python3.13/site-packages (from reportlab) (11.0.0)\n",
      "Collecting chardet (from reportlab)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3.13/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/lib64/python3.13/site-packages (from nltk) (2024.9.11)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/hidan/.local/lib/python3.13/site-packages (from scikit-learn) (2.2.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.13/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Using cached googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Using cached reportlab-4.3.0-py3-none-any.whl (1.9 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.2 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, PyPDF2, joblib, chardet, scikit-learn, reportlab, nltk, googlesearch-python\n",
      "Successfully installed PyPDF2-3.0.1 chardet-5.2.0 googlesearch-python-1.3.0 joblib-1.4.2 nltk-3.9.1 reportlab-4.3.0 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests PyPDF2 googlesearch-python reportlab nltk scikit-learn pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
