{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora, models\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import requests\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from googlesearch import search\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, PageBreak, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Se necess√°rio, redefina sys.argv para evitar conflitos com argumentos do kernel:\n",
    "sys.argv = ['ipykernel_launcher.py', 'python', '-n', '5', '-q', 'alta']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('subject', nargs='?', default='assunto_padrao', help='Assunto para pesquisa')\n",
    "parser.add_argument('-n', '--num-pdfs', type=int, default=10, help='N√∫mero de PDFs')\n",
    "parser.add_argument('-q', '--quality', type=str, default='alta', help='Qualidade da pesquisa')  # Agora sempre string!\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"Subject: {args.subject}\")\n",
    "print(f\"N√∫mero de PDFs: {args.num_pdfs}\")\n",
    "print(f\"Qualidade: {args.quality} (Tipo: {type(args.quality)})\") \n",
    "\n",
    "class AdvancedKnowledgeAggregator:\n",
    "    def __init__(self, subject):\n",
    "        self.subject = subject\n",
    "        self.base_dir = Path(\"knowledge_books\") / subject.replace(\" \", \"_\")\n",
    "        self.raw_pdfs = self.base_dir / \"raw_pdfs\"\n",
    "        self.processed_texts = []\n",
    "        self.metadata = []\n",
    "        self.structure = {}\n",
    "        self.quality_scores = {}\n",
    "        \n",
    "        self._setup_directories()\n",
    "        self._load_config()\n",
    "\n",
    "    def _setup_directories(self):\n",
    "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.raw_pdfs.mkdir(exist_ok=True)\n",
    "\n",
    "    def _load_config(self):\n",
    "        config_path = Path(\"config.yaml\")\n",
    "        if config_path.exists():\n",
    "            with open(config_path) as f:\n",
    "                self.config = yaml.safe_load(f)\n",
    "        else:\n",
    "            self.config = {\n",
    "                'structure': {\n",
    "                    'Introdu√ß√£o': ['Hist√≥rico', 'Conceitos B√°sicos'],\n",
    "                    'Desenvolvimento': ['T√©cnicas', 'Aplica√ß√µes'],\n",
    "                    'Conclus√£o': ['Resumo', 'Pr√≥ximos Passos']\n",
    "                },\n",
    "                'apis': {\n",
    "                    'crossref': {'enable': True, 'max_results': 5},\n",
    "                    'arxiv': {'enable': False}\n",
    "                }\n",
    "            }\n",
    "\n",
    "    def search_content(self, num_files=15):\n",
    "        \"\"\"Busca conte√∫do em m√∫ltiplas fontes\"\"\"\n",
    "        self._search_web_pdfs(num_files)\n",
    "        if self.config['apis']['crossref']['enable']:\n",
    "            self._search_academic_papers()\n",
    "\n",
    "    def _search_web_pdfs(self, num_files):\n",
    "        query = f\"{self.subject} filetype:pdf\"\n",
    "        try:\n",
    "            results = search(\n",
    "                query,\n",
    "                num_results=num_files,\n",
    "                lang='pt' if 'brasil' in self.subject.lower() else 'en',\n",
    "                user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
    "            )\n",
    "            \n",
    "            for url in results:\n",
    "                if url.endswith('.pdf'):\n",
    "                    self._download_pdf(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na pesquisa web: {e}\")\n",
    "\n",
    "    def _search_academic_papers(self):\n",
    "        \"\"\"Integra√ß√£o com API CrossRef para artigos acad√™micos\"\"\"\n",
    "        url = \"https://api.crossref.org/works\"\n",
    "        params = {\n",
    "            'query': self.subject,\n",
    "            'filter': 'type:journal-article',\n",
    "            'rows': self.config['apis']['crossref']['max_results']\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=15)\n",
    "            if response.status_code == 200:\n",
    "                for item in response.json()['message']['items']:\n",
    "                    if 'link' in item and item['link'][0]['content-type'] == 'application/pdf':\n",
    "                        self._download_pdf(item['link'][0]['URL'])\n",
    "                        self.metadata.append({\n",
    "                            'title': item.get('title', [''])[0],\n",
    "                            'authors': [author['given'] + ' ' + author['family'] for author in item.get('author', [])],\n",
    "                            'doi': item.get('DOI', ''),\n",
    "                            'year': item.get('created', {}).get('date-parts', [[2000]])[0][0]\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na API CrossRef: {e}\")\n",
    "\n",
    "    def _download_pdf(self, url):\n",
    "        \"\"\"Sistema de download com verifica√ß√£o de qualidade\"\"\"\n",
    "        try:\n",
    "            filename = url.split('/')[-1][:100] + \".pdf\"\n",
    "            filepath = self.raw_pdfs / filename\n",
    "            \n",
    "            if filepath.exists():\n",
    "                return\n",
    "\n",
    "            response = requests.get(url, stream=True, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                self._assess_quality(filepath)\n",
    "                print(f\"‚úÖ Baixado: {filename} [Score: {self.quality_scores.get(filename, 0):.1f}]\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar {url}: {e}\")\n",
    "\n",
    "    def _assess_quality(self, filepath):\n",
    "        \"\"\"Avalia√ß√£o de qualidade do PDF\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                reader = PdfReader(f)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages[:5])\n",
    "                \n",
    "                # Crit√©rios de qualidade\n",
    "                length_score = min(len(text)/1000, 5)\n",
    "                keyword_score = sum(1 for w in ['introduction', 'method', 'conclusion'] if w in text.lower())\n",
    "                self.quality_scores[filepath.name] = length_score + keyword_score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro na avalia√ß√£o de qualidade: {e}\")\n",
    "            self.quality_scores[filepath.name] = 0\n",
    "\n",
    "    def process_content(self):\n",
    "        \"\"\"Processamento avan√ßado com limpeza e an√°lise\"\"\"\n",
    "        for pdf_file in sorted(self.raw_pdfs.glob(\"*.pdf\"), \n",
    "                             key=lambda x: self.quality_scores.get(x.name, 0), \n",
    "                             reverse=True):\n",
    "            try:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                text = \" \".join(page.extract_text() or '' for page in reader.pages)\n",
    "                clean_text = self._clean_text(text)\n",
    "                \n",
    "                if len(clean_text) > 500:  # Ignorar PDFs sem texto √∫til\n",
    "                    self.processed_texts.append(clean_text)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar {pdf_file.name}: {e}\")\n",
    "\n",
    "        if self.processed_texts:\n",
    "            self._analyze_topics()\n",
    "            self._generate_index()\n",
    "\n",
    "    def _clean_text(self, text):\n",
    "        \"\"\"Limpeza avan√ßada do texto\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "        text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)  # Remover repeti√ß√µes\n",
    "        return text.lower()\n",
    "\n",
    "   \n",
    "\n",
    "def _analyze_topics(self):\n",
    "    # Vetoriza√ß√£o dos textos\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    tf = vectorizer.fit_transform(self.processed_texts)\n",
    "    \n",
    "    # Aplica√ß√£o do LDA\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=5,  # N√∫mero de t√≥picos\n",
    "        max_iter=10,\n",
    "        learning_method='online',\n",
    "        random_state=42\n",
    "    )\n",
    "    lda.fit(tf)\n",
    "    \n",
    "    # Extra√ß√£o dos t√≥picos\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    self.structure = {}\n",
    "    \n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        top_terms = [feature_names[i] for i in topic.argsort()[:-4:-1]]  # Top 3 termos\n",
    "        self.structure[f\"Cap√≠tulo {idx+1}: {' '.join(top_terms)}\"] = []\n",
    "\n",
    "    def _generate_index(self):\n",
    "        \"\"\"Gera√ß√£o de √≠ndice remissivo\"\"\"\n",
    "        all_text = ' '.join(self.processed_texts)\n",
    "        words = nltk.word_tokenize(all_text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [w for w in words if w not in stop_words and len(w) > 4 and w.isalpha()]\n",
    "        \n",
    "        self.index = FreqDist(filtered_words).most_common(50)\n",
    "\n",
    "    def generate_book(self):\n",
    "        \"\"\"Gera√ß√£o do PDF com estrutura profissional\"\"\"\n",
    "        output_file = self.base_dir / f\"{self.subject}_{datetime.now().strftime('%Y%m%d')}.pdf\"\n",
    "        \n",
    "        doc = SimpleDocTemplate(str(output_file), pagesize=letter)\n",
    "        styles = self._create_styles()\n",
    "        story = []\n",
    "        \n",
    "        # Capa\n",
    "        story += self._create_cover(styles)\n",
    "        \n",
    "        # Metadados\n",
    "        story += self._create_metadata_table(styles)\n",
    "        \n",
    "        # Sum√°rio\n",
    "        story += self._create_toc(styles)\n",
    "        \n",
    "        # Conte√∫do principal\n",
    "        story += self._create_main_content(styles)\n",
    "        \n",
    "        # √çndice Remissivo\n",
    "        story += self._create_index(styles)\n",
    "        \n",
    "        doc.build(story)\n",
    "        print(f\"\\nüìï Livro gerado com sucesso em: {output_file}\")\n",
    "\n",
    "    def _create_styles(self):\n",
    "        \"\"\"Configura√ß√£o de estilos avan√ßados\"\"\"\n",
    "        styles = getSampleStyleSheet()\n",
    "        styles.add(ParagraphStyle(\n",
    "            name='ChapterTitle',\n",
    "            fontSize=16,\n",
    "            leading=18,\n",
    "            spaceAfter=20,\n",
    "            textColor=colors.darkblue\n",
    "        ))\n",
    "        return styles\n",
    "\n",
    "    def _create_cover(self, styles):\n",
    "        \"\"\"Cria√ß√£o da capa profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(f\"<b>{self.subject.upper()}</b>\", styles['Title']))\n",
    "        elements.append(Paragraph(\"<br/><br/>Relat√≥rio Gerado Automaticamente<br/>por Knowledge Aggregator 2.0\", styles['Italic']))\n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_metadata_table(self, styles):\n",
    "        \"\"\"Tabela de metadados dos artigos\"\"\"\n",
    "        data = [['T√≠tulo', 'Autores', 'Ano', 'DOI']]\n",
    "        for meta in self.metadata:\n",
    "            data.append([\n",
    "                meta['title'][:50],\n",
    "                ', '.join(meta['authors'][:3]),\n",
    "                str(meta['year']),\n",
    "                meta['doi'][:20]\n",
    "            ])\n",
    "        \n",
    "        table = Table(data, colWidths=[120, 120, 50, 100])\n",
    "        table.setStyle(TableStyle([\n",
    "            ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),\n",
    "            ('GRID', (0,0), (-1,-1), 1, colors.black),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 8)\n",
    "        ]))\n",
    "        \n",
    "        return [Paragraph(\"<b>Fontes Acad√™micas Utilizadas</b>\", styles['Heading2']), table, PageBreak()]\n",
    "\n",
    "    def _create_toc(self, styles):\n",
    "        \"\"\"Sum√°rio autom√°tico com numera√ß√£o\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"Sum√°rio\", styles['Heading1']))\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(f\"‚Ä¢ {chapter}\", styles['Normal']))\n",
    "        \n",
    "        elements.append(PageBreak())\n",
    "        return elements\n",
    "\n",
    "    def _create_main_content(self, styles):\n",
    "        \"\"\"Conte√∫do estruturado com an√°lise sem√¢ntica\"\"\"\n",
    "        elements = []\n",
    "        vectorizer = TfidfVectorizer(max_features=1000)\n",
    "        tfidf_matrix = vectorizer.fit_transform(self.processed_texts)\n",
    "        \n",
    "        for chapter in self.structure:\n",
    "            elements.append(Paragraph(chapter, styles['ChapterTitle']))\n",
    "            \n",
    "            # Selecionar conte√∫do mais relevante\n",
    "            chapter_keywords = chapter.lower().split()[1:]\n",
    "            relevant_content = max(\n",
    "                self.processed_texts,\n",
    "                key=lambda x: sum(1 for kw in chapter_keywords if kw in x)\n",
    "            )\n",
    "            \n",
    "            elements.append(Paragraph(relevant_content[:1500] + \"...\", styles['Normal']))\n",
    "            elements.append(PageBreak())\n",
    "        \n",
    "        return elements\n",
    "\n",
    "    def _create_index(self, styles):\n",
    "        \"\"\"√çndice remissivo profissional\"\"\"\n",
    "        elements = []\n",
    "        elements.append(Paragraph(\"√çndice Remissivo\", styles['Heading1']))\n",
    "        \n",
    "        index_items = []\n",
    "        for term, freq in self.index:\n",
    "            index_items.append(f\"{term} ({freq})\")\n",
    "        \n",
    "        columns = 3\n",
    "        table_data = []\n",
    "        for i in range(0, len(index_items), columns):\n",
    "            table_data.append(index_items[i:i+columns])\n",
    "        \n",
    "        table = Table(table_data)\n",
    "        table.setStyle(TableStyle([\n",
    "            ('ALIGN', (0,0), (-1,-1), 'LEFT'),\n",
    "            ('FONTSIZE', (0,0), (-1,-1), 9)\n",
    "        ]))\n",
    "        \n",
    "        return [table]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Advanced Knowledge Aggregator\")\n",
    "    parser.add_argument(\"subject\", type=str, help=\"Assunto principal do livro\")\n",
    "    parser.add_argument(\"-n\", \"--num-pdfs\", type=int, default=10, help=\"N√∫mero de PDFs para baixar\")\n",
    "    parser.add_argument(\"-q\", \"--quality\", type=float, default=3.0, help=\"Limite m√≠nimo de qualidade (0-5)\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print(f\"üöÄ Iniciando agrega√ß√£o de conhecimento sobre: {args.subject}\")\n",
    "    aggregator = AdvancedKnowledgeAggregator(args.subject)\n",
    "    \n",
    "    print(\"\\nüîç Buscando conte√∫do relevante...\")\n",
    "    aggregator.search_content(args.num_pdfs)\n",
    "    \n",
    "    print(\"\\nüß† Processando e analisando conte√∫do...\")\n",
    "    aggregator.process_content()\n",
    "    \n",
    "    if aggregator.processed_texts:\n",
    "        print(\"\\nüìö Gerando livro estruturado...\")\n",
    "        aggregator.generate_book()\n",
    "    else:\n",
    "        print(\"‚ùå Nenhum conte√∫do v√°lido encontrado para gera√ß√£o do livro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/lib/python3.13/site-packages (2.32.3)\n",
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting googlesearch-python\n",
      "  Using cached googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting reportlab\n",
      "  Using cached reportlab-4.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/lib64/python3.13/site-packages (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/lib/python3.13/site-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/lib64/python3.13/site-packages (from reportlab) (11.0.0)\n",
      "Collecting chardet (from reportlab)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: click in /usr/lib/python3.13/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/lib64/python3.13/site-packages (from nltk) (2024.9.11)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/hidan/.local/lib/python3.13/site-packages (from scikit-learn) (2.2.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.13/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Using cached googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Using cached reportlab-4.3.0-py3-none-any.whl (1.9 MB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached scikit_learn-1.6.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.15.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.2 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, PyPDF2, joblib, chardet, scikit-learn, reportlab, nltk, googlesearch-python\n",
      "Successfully installed PyPDF2-3.0.1 chardet-5.2.0 googlesearch-python-1.3.0 joblib-1.4.2 nltk-3.9.1 reportlab-4.3.0 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests PyPDF2 googlesearch-python reportlab nltk scikit-learn pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
